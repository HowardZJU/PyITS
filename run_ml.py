import os
import random
import sys

import numpy as np
import setproctitle
import torch

from data_provider import DATA_DICT
from estimator import ML_ESTIMATOR_DICT
from models import MODEL_DICT
from utils.argument_parser import parse_arguments
from utils.logger import Logger


def load_data(args, logger):
    ## build dataset and generate data
    dataset = DATA_DICT[args.data](args, logger)
    args = dataset.generate_data(task_name=args.task_name)
    return dataset, args


def load_model(args):
    model_cls = MODEL_DICT[args.model].Model
    if args.task_name in model_cls.supported_tasks:
        model = model_cls(args)
    else:
        raise ValueError(f"Model {args.model} does not support task {args.task_name}")
    return model


def load_estimator(args):
    Estimator = ML_ESTIMATOR_DICT[args.task_name]
    return Estimator


def load_device(gpu_ids):
    ## can not use torch.cuda.is_available() before os.environ["CUDA_VISIBLE_DEVICES"] = gpu_ids, otherwise it will not work
    use_gpu = True if gpu_ids != '-1' else False
    device = torch.device('cpu')

    if use_gpu:
        args.device_ids = [i for i, id_ in enumerate(gpu_ids.replace(' ', '').split(','))]
        os.environ["CUDA_VISIBLE_DEVICES"] = gpu_ids

        if torch.cuda.is_available():
            device = torch.device(f'cuda:0')  # Primary GPU
            if len(args.device_ids) > 1:
                logger.info(f'Using multiple GPUs: {args.gpu_ids}', color='red')
            else:
                logger.info(f'Using single GPU: cuda:{gpu_ids}', color='red')
        else:
            logger.info('Using CPU', color='red')
    else:
        logger.info('Using CPU', color='red')

    return device


def seed_everything(seed):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    np.random.seed(seed)


if __name__ == '__main__':
    args = parse_arguments()

    ## fix seed
    seed_everything(args.fix_seed)

    ## build logger
    logger = Logger(log_dir=args.save_dir, remove_old=args.remove_log)

    if not args.rerun and os.path.exists(os.path.join(args.save_dir, "test.yaml")):
        logger.info(f"Setting {args.setting} already run, skip", color='red')
        sys.exit()

    ## gpu/cpu setting
    args.device = load_device(gpu_ids=args.gpu_ids)

    ## build dataset and generate data
    ## since data type generated by different tasks are different, we generate data in a group wise manner, and fix the first item in a group as x and the second item as y
    dataset, args = load_data(args, logger)
    train_data = dataset.get_data(flag='train')
    logger.info(f"x_train: {train_data[0].shape}, y_train: {train_data[1].shape}", color='red')

    ## build model
    model = load_model(args)

    ## build estimator
    setproctitle.setproctitle(args.task_name)
    Estimator = load_estimator(args)
    estimator = Estimator(args, dataset=dataset, model=model, device=args.device, logger=logger)

    ## training and testing
    if args.is_training:
        logger.info(f'Start training: {args.setting}', color='red')
        estimator.fit()

        logger.info(f'Testing: {args.setting}', color='red')
        estimator.test()

    else:
        logger.info(f'Testing: {args.setting}', color='red')
        estimator.test(test=1)

    torch.cuda.empty_cache()
